{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc37085-17bf-4965-964d-dea712f04b76",
   "metadata": {},
   "source": [
    "## HW2 Probelm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f9fa60-80bd-461b-a7f4-f2b7ea1a9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch import nn, optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision.models import resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11220f1b-b8f3-45b9-a4de-f7e4a196e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
    "\n",
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c43764b-9339-43c0-9df3-1aaa5e4f837e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ebbbb97-d968-4af8-8a98-f177ada04640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config\n",
    "gpu_counts = [1]\n",
    "batch_sizes = [16, 32, 128, 512]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07d7355-9343-4d3f-a935-5f1e2c83fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "train_ds = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3e96cd-5fae-4936-bdae-da70569208ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running on 1 GPU(s) ===\n",
      "Batch=16   →  34.83 sec\n",
      "Batch=32   →  20.10 sec\n",
      "Batch=128  →  17.70 sec\n",
      "Batch=512  →  19.91 sec\n",
      "\n",
      "Summary Results:\n",
      "GPUs | Batch | Time (s)\n",
      "--------------------------\n",
      "   1 |    16 |    34.83\n",
      "   1 |    32 |    20.10\n",
      "   1 |   128 |    17.70\n",
      "   1 |   512 |    19.91\n"
     ]
    }
   ],
   "source": [
    "for gpus in gpu_counts:\n",
    "    print(f\"\\n=== Running on {gpus} GPU(s) ===\")\n",
    "    for bs in batch_sizes:\n",
    "        try:\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_ds, batch_size=bs, shuffle=True, num_workers=2\n",
    "            )\n",
    "            net = ResNet18().to(device)\n",
    "            if gpus > 1:\n",
    "                net = nn.DataParallel(net)  # will leverage available GPUs\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Warm-up epoch\n",
    "            net.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                criterion(net(xb), yb).backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Timed epoch\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                criterion(net(xb), yb).backward()\n",
    "                optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.perf_counter() - start\n",
    "\n",
    "            results[(gpus, bs)] = elapsed\n",
    "            print(f\"Batch={bs:<4} → {elapsed:6.2f} sec\")\n",
    "        except RuntimeError as e:\n",
    "            results[(gpus, bs)] = None\n",
    "            print(f\"Batch={bs:<4} → OOM or Error: {str(e).splitlines()[0]}\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nSummary Results:\")\n",
    "print(f\"{'GPUs':>4} | {'Batch':>5} | {'Time (s)':>8}\")\n",
    "print(\"-\" * 26)\n",
    "for (gpus, bs), t in results.items():\n",
    "    time_str = f\"{t:.2f}\" if t is not None else \"OOM\"\n",
    "    print(f\"{gpus:>4} | {bs:>5} | {time_str:>8}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f826cd0-9003-4e33-810e-f919418d7b1f",
   "metadata": {},
   "source": [
    "## 2 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4defe15a-a54a-489e-934a-262e0b75c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from models.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c504dd10-d765-48fe-94e1-a0eeeca6786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
    "\n",
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a375ebf-ea0b-4cee-b340-e9b0372bd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "gpu_counts = [2]\n",
    "batch_sizes = [16, 32, 128, 512]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare dataset once\n",
    "transform = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "train_ds = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34cc408b-014c-4b35-8e7d-dcd927d9f34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running on 2 GPU(s) ===\n",
      "Batch=16   → 171.60 sec\n",
      "Batch=32   →  87.07 sec\n",
      "Batch=128  →  22.66 sec\n",
      "Batch=512  →  10.12 sec\n"
     ]
    }
   ],
   "source": [
    "for gpus in gpu_counts:\n",
    "    print(f\"\\n=== Running on {gpus} GPU(s) ===\")\n",
    "    for bs in batch_sizes:\n",
    "        try:\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_ds, batch_size=bs, shuffle=True, num_workers=2\n",
    "            )\n",
    "            net = ResNet18().to(device)\n",
    "            if gpus > 1:\n",
    "                net = nn.DataParallel(net)  # will leverage available GPUs\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Warm-up epoch\n",
    "            net.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                criterion(net(xb), yb).backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Timed epoch\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                criterion(net(xb), yb).backward()\n",
    "                optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.perf_counter() - start\n",
    "\n",
    "            results[(gpus, bs)] = elapsed\n",
    "            print(f\"Batch={bs:<4} → {elapsed:6.2f} sec\")\n",
    "        except RuntimeError as e:\n",
    "            results[(gpus, bs)] = None\n",
    "            print(f\"Batch={bs:<4} → OOM or Error: {str(e).splitlines()[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72e7130-333d-4970-9803-e3cdb0150166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary Results:\n",
      "GPUs | Batch | Time (s)\n",
      "--------------------------\n",
      "   2 |    16 |   171.60\n",
      "   2 |    32 |    87.07\n",
      "   2 |   128 |    22.66\n",
      "   2 |   512 |    10.12\n"
     ]
    }
   ],
   "source": [
    "# Display summary table\n",
    "print(\"\\nSummary Results:\")\n",
    "print(f\"{'GPUs':>4} | {'Batch':>5} | {'Time (s)':>8}\")\n",
    "print(\"-\" * 26)\n",
    "for (gpus, bs), t in results.items():\n",
    "    time_str = f\"{t:.2f}\" if t is not None else \"OOM\"\n",
    "    print(f\"{gpus:>4} | {bs:>5} | {time_str:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c1961-b628-4ffe-9b6c-413eccee1e38",
   "metadata": {},
   "source": [
    "## 4GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a3799e-d8c8-4f0e-946d-c649a1691fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e42009-56dd-49a3-8e3e-5f9563e2dd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running on 4 GPU(s) ===\n",
      "Batch=16   → 172.49 sec\n",
      "Batch=32   →  91.34 sec\n",
      "Batch=128  →  22.02 sec\n",
      "Batch=512  →  10.13 sec\n",
      "\n",
      "Summary Results:\n",
      "GPUs | Batch | Time (s)\n",
      "--------------------------\n",
      "   4 |    16 |   172.49\n",
      "   4 |    32 |    91.34\n",
      "   4 |   128 |    22.02\n",
      "   4 |   512 |    10.13\n"
     ]
    }
   ],
   "source": [
    "#ResNet from https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py\n",
    "\n",
    "'''ResNet in PyTorch.\n",
    "\n",
    "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
    "\n",
    "Reference:\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
    "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "# Configuration\n",
    "gpu_counts = [4]\n",
    "batch_sizes = [16, 32, 128, 512]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare dataset once\n",
    "transform = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "train_ds = torchvision.datasets.CIFAR10('./data', train=True, download=True, transform=transform)\n",
    "\n",
    "results = {}\n",
    "\n",
    "for gpus in gpu_counts:\n",
    "    print(f\"\\n=== Running on {gpus} GPU(s) ===\")\n",
    "    for bs in batch_sizes:\n",
    "        try:\n",
    "            train_loader = torch.utils.data.DataLoader(\n",
    "                train_ds, batch_size=bs, shuffle=True, num_workers=2\n",
    "            )\n",
    "            net = ResNet18().to(device)\n",
    "            if gpus > 1:\n",
    "                net = nn.DataParallel(net)  # will leverage available GPUs\n",
    "            optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            # Warm-up epoch\n",
    "            net.train()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                criterion(net(xb), yb).backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Timed epoch\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                criterion(net(xb), yb).backward()\n",
    "                optimizer.step()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.perf_counter() - start\n",
    "\n",
    "            results[(gpus, bs)] = elapsed\n",
    "            print(f\"Batch={bs:<4} → {elapsed:6.2f} sec\")\n",
    "        except RuntimeError as e:\n",
    "            results[(gpus, bs)] = None\n",
    "            print(f\"Batch={bs:<4} → OOM or Error: {str(e).splitlines()[0]}\")\n",
    "\n",
    "# Display summary table\n",
    "print(\"\\nSummary Results:\")\n",
    "print(f\"{'GPUs':>4} | {'Batch':>5} | {'Time (s)':>8}\")\n",
    "print(\"-\" * 26)\n",
    "for (gpus, bs), t in results.items():\n",
    "    time_str = f\"{t:.2f}\" if t is not None else \"OOM\"\n",
    "    print(f\"{gpus:>4} | {bs:>5} | {time_str:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf702f2-65e2-4663-8725-7739aa8d26c7",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710815e1-5756-46d5-bf35-8a31510e90cb",
   "metadata": {},
   "source": [
    "\n",
    "Table1:\n",
    "| Batch | T₁ (s) | T₂ (s) | S₂ = T₁/T₂ | T₄ (s) | S₄ = T₁/T₄ |\n",
    "| ----- | ------ | ------ | ---------- | ------ | ---------- |\n",
    "| 16    | 34.83  | 171.60 | 0.20       | 172.49 | 0.20       |\n",
    "| 32    | 20.10  | 87.07  | 0.23       | 91.34  | 0.22       |\n",
    "| 128   | 17.70  | 22.66  | 0.78       | 22.02  | 0.80       |\n",
    "| 512   | 19.91  | 10.12  | 1.97       | 10.13  | 1.97       |\n",
    "\n",
    "\n",
    "table2:\n",
    "|       | Batch-size 16 per GPU | Batch-size 32 per GPU | Batch-size 128 per GPU | Batch-size 512 per GPU |         |      |         |             |\n",
    "| ----- | --------------------- | --------------------- | ---------------------- | ---------------------- | ------- | ---- | ------- | ----------- |\n",
    "|       | Compute               | Comm                  | Compute                | Comm                   | Compute | Comm | Compute | Comm        |\n",
    "| 2-GPU | 34.83                 | 136.77                | 20.10                  | 66.97                  | 17.70   | 4.96 | 19.91   | 0 (clipped) |\n",
    "| 4-GPU | 34.83                 | 137.66                | 20.10                  | 71.24                  | 17.70   | 4.32 | 19.91   | 0 (clipped) |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fc6c1-e82a-40b7-b138-79a06140fb1b",
   "metadata": {},
   "source": [
    "table 3:\n",
    "Bandwidth Utilization (GB/s)= \n",
    "Communication time (s) /\n",
    "Data size to communicate (GB)\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f1cfb-7a14-4b2c-bf80-8d4318c22cd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f483616-23da-41a3-8c3a-c9b07ced80e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet-18 parameters: 11173962 (0.0447 GB per sync)\n"
     ]
    }
   ],
   "source": [
    "# Count total model parameters (float32, 4 bytes each)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "num_params = count_parameters(ResNet18())\n",
    "data_per_sync_gb = num_params * 4 / 1e9  # GB per sync\n",
    "print(f\"ResNet-18 parameters: {num_params} ({data_per_sync_gb:.4f} GB per sync)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "634c1c4c-8cfc-4158-a2bf-efb1529e3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bandwidth Utilization Summary ===\n",
      "GPUs | Batch |  Comm(s) |  BW(GB/s)\n",
      "------------------------------------\n",
      "   4 |    16 |      N/A |       N/A\n",
      "   4 |    32 |      N/A |       N/A\n",
      "   4 |   128 |      N/A |       N/A\n",
      "   4 |   512 |      N/A |       N/A\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Number of training samples in CIFAR-10\n",
    "num_train_samples = len(train_ds)\n",
    "\n",
    "print(\"\\n=== Bandwidth Utilization Summary ===\")\n",
    "print(f\"{'GPUs':>4} | {'Batch':>5} | {'Comm(s)':>8} | {'BW(GB/s)':>9}\")\n",
    "print(\"-\" * 36)\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    t1 = results.get((1, bs))\n",
    "    num_batches = math.ceil(num_train_samples / bs)\n",
    "    total_data_gb = data_per_sync_gb * num_batches\n",
    "\n",
    "    for gpus in gpu_counts:\n",
    "        if gpus == 1:\n",
    "            continue  # no communication for single GPU\n",
    "        tN = results.get((gpus, bs))\n",
    "        if t1 is None or tN is None:\n",
    "            comm_time = bw = None\n",
    "        else:\n",
    "            comm_time = max(0, tN - t1)  # max to avoid negative\n",
    "            bw = total_data_gb / comm_time if comm_time > 0 else None\n",
    "        comm_str = f\"{comm_time:.2f}\" if comm_time is not None else \"N/A\"\n",
    "        bw_str = f\"{bw:.3f}\" if bw is not None else \"N/A\"\n",
    "        print(f\"{gpus:>4} | {bs:>5} | {comm_str:>8} | {bw_str:>9}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de88fe-1147-4d02-b97b-6d8f258abb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
