{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN24pLA/VKubpKkPzHXB7zp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# HW2 Problem9"],"metadata":{"id":"iix5Ue147yC0"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"02J9sYIz7xV5","executionInfo":{"status":"ok","timestamp":1751748655292,"user_tz":240,"elapsed":25739,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"804feefe-6a14-487a-c7d3-a21df965a612"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}],"source":["import getpass\n","import os\n","\n","if not os.environ.get(\"OPENAI_API_KEY\"):\n","    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n"]},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WvFCBrlPZ-PB","executionInfo":{"status":"ok","timestamp":1751748663166,"user_tz":240,"elapsed":6233,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"ece91361-871b-42b8-d0e6-127c0f00fbe2"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}]},{"cell_type":"code","source":["%pip install -qU langchain-openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8u3Y6gMvaWP-","executionInfo":{"status":"ok","timestamp":1751748671197,"user_tz":240,"elapsed":5289,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"a75ce7c0-e1c0-4d8c-e56a-93da336bf754"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/70.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(\n","    model=\"gpt-4o\",\n","    temperature=0,\n","    max_tokens=None,\n","    timeout=None,\n","    max_retries=2,\n","    # api_key=\"...\",  # if you prefer to pass api key in directly instaed of using env vars\n","    # base_url=\"...\",\n","    # organization=\"...\",\n","    # other params...\n",")"],"metadata":{"id":"GFIZBD9MbLNj","executionInfo":{"status":"ok","timestamp":1751748885380,"user_tz":240,"elapsed":3940,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","\n","system_message = \"Classify the sentiment of the following text as Positive, Negative, or Neutral. Only reply with one of: Positive, Negative, or Neutral.\"\n","prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\", system_message),\n","    (\"human\", \"{input}\")\n","])\n"],"metadata":{"id":"A3AdORW3ifPB","executionInfo":{"status":"ok","timestamp":1751726547765,"user_tz":240,"elapsed":12,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## 1A\n"],"metadata":{"id":"1ZL7NCw38ceZ"}},{"cell_type":"code","source":["texts = [\n","    \"The movie was absolutely fantastic and exceeded my expectations!\",\n","    \"I'm not sure how I feel about this product.\",\n","    \"This service was terrible and completely disappointing.\"\n","]\n","\n","# Prepare your LLM (replace with your model init code, e.g. gpt-4o)\n","from langchain_openai import ChatOpenAI\n","llm = ChatOpenAI(model=\"gpt-4o\")  # or \"gpt-4\", \"gpt-4-turbo\", etc.\n","\n","# Run the model for each example\n","for text in texts:\n","    prompt = prompt_template.invoke({\"input\": text})\n","    result = llm.invoke(prompt.messages)\n","    print(f\"Text: {text}\\nSentiment: {result.content}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B17wmmZn8fNn","executionInfo":{"status":"ok","timestamp":1751726551326,"user_tz":240,"elapsed":3552,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"0695df7d-1517-403f-c6e9-2902713b8160"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: The movie was absolutely fantastic and exceeded my expectations!\n","Sentiment: Positive\n","\n","Text: I'm not sure how I feel about this product.\n","Sentiment: Neutral\n","\n","Text: This service was terrible and completely disappointing.\n","Sentiment: Negative\n","\n"]}]},{"cell_type":"markdown","source":["## 1B"],"metadata":{"id":"DYQqaS3587Nk"}},{"cell_type":"code","source":["system_fewshot = \"\"\"Classify the sentiment of the following text as Positive, Negative, or Neutral. Only reply with one of: Positive, Negative, or Neutral.\n","\n","Examples:\n","Text: \"I loved this product, it works great!\"\n","Sentiment: Positive\n","\n","Text: \"It was okay, nothing special.\"\n","Sentiment: Neutral\n","\n","Text: \"I hated the service, it was awful.\"\n","Sentiment: Negative\n","\"\"\"\n","prompt_template_few = ChatPromptTemplate.from_messages([\n","    (\"system\", system_fewshot),\n","    (\"human\", \"{input}\")\n","])\n"],"metadata":{"id":"sxbhNvvp89vQ","executionInfo":{"status":"ok","timestamp":1751726551349,"user_tz":240,"elapsed":19,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["for text in texts:\n","    prompt = prompt_template_few.invoke({\"input\": text})\n","    result = llm.invoke(prompt.messages)\n","    print(f\"Text: {text}\\nSentiment: {result.content}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXeZNsmM9Z3S","executionInfo":{"status":"ok","timestamp":1751726553938,"user_tz":240,"elapsed":2587,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"c61415cf-13c1-45b2-a68b-d3ec379b2f82"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: The movie was absolutely fantastic and exceeded my expectations!\n","Sentiment: Positive\n","\n","Text: I'm not sure how I feel about this product.\n","Sentiment: Neutral\n","\n","Text: This service was terrible and completely disappointing.\n","Sentiment: Negative\n","\n"]}]},{"cell_type":"markdown","source":["## 1C\n","From this result we are very hard to tell the difference. But usualy a few-shot prompt would do better. Few-shot prompting gives the model concrete examples of what you want—including the style, format, and boundaries for answers.\n","In zero-shot, the model has to guess your intent based only on a generic instruction.\n","\n","In few-shot, you “anchor” the model’s behavior with labeled data points, making it less likely to misinterpret your prompt.\n","\n","Few-shot examples can include tricky or ambiguous cases, so the model learns how to respond to them.\n","\n","It also teaches the model exactly how you want the output formatted (e.g., “Sentiment: Positive” vs just “Positive”).\n","\n","LLMs were trained on huge datasets with many different prompt styles.\n","\n","By providing a few labeled examples, you help the model activate the correct pattern or template in its parameters, making the answer more reliable.\n","\n"],"metadata":{"id":"Z4xbdIHl90yk"}},{"cell_type":"markdown","source":["## 2A"],"metadata":{"id":"vWgE92XJBqET"}},{"cell_type":"code","source":["# 1. CoT Prompt\n","cot_system = (\n","    \"Let's solve math problems step by step. Show all calculations and reasoning.\"\n",")\n","cot_human = (\n","    \"A bakery sells cupcakes for $3 each and cookies for $2 each. \"\n","    \"If Sarah buys 4 cupcakes and 6 cookies, and pays with a $50 bill, how much change will she receive?\"\n",")\n","\n","cot_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", cot_system),\n","    (\"human\", cot_human)\n","])\n","\n","llm = ChatOpenAI(model=\"gpt-4o\")  # Or your preferred model\n","\n","# Run CoT (CORRECT WAY)\n","prompt = cot_prompt.invoke({})\n","cot_result = llm.invoke(prompt.messages)\n","print(\"Chain-of-Thought Result:\\n\", cot_result.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5IiPA62LCKI5","executionInfo":{"status":"ok","timestamp":1751726613694,"user_tz":240,"elapsed":3349,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"fb102be9-611a-40e6-f2cc-0247a707dbc7"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Chain-of-Thought Result:\n"," To find out how much change Sarah will receive, we first need to calculate the total cost of the cupcakes and cookies she buys.\n","\n","1. **Calculate the cost of the cupcakes:**\n","   Sarah buys 4 cupcakes, and each cupcake costs $3.\n","   \\[\n","   \\text{Cost of cupcakes} = 4 \\times 3 = 12 \\text{ dollars}\n","   \\]\n","\n","2. **Calculate the cost of the cookies:**\n","   Sarah buys 6 cookies, and each cookie costs $2.\n","   \\[\n","   \\text{Cost of cookies} = 6 \\times 2 = 12 \\text{ dollars}\n","   \\]\n","\n","3. **Find the total cost:**\n","   Add the cost of cupcakes and the cost of cookies.\n","   \\[\n","   \\text{Total cost} = \\text{Cost of cupcakes} + \\text{Cost of cookies} = 12 + 12 = 24 \\text{ dollars}\n","   \\]\n","\n","4. **Calculate the change:**\n","   Sarah pays with a $50 bill. Subtract the total cost from the amount she pays to find the change.\n","   \\[\n","   \\text{Change} = 50 - 24 = 26 \\text{ dollars}\n","   \\]\n","\n","Therefore, Sarah will receive \\(\\boxed{26}\\) dollars in change.\n"]}]},{"cell_type":"markdown","source":["## 2B\n"],"metadata":{"id":"hRfT4T2GDHSc"}},{"cell_type":"code","source":["direct_system = (\n","    \"Answer the following math question as concisely as possible.\"\n",")\n","direct_human = cot_human  # same question\n","\n","direct_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", direct_system),\n","    (\"human\", direct_human)\n","])\n","\n","# Run Direct\n","direct_prompt = direct_prompt.invoke({})\n","direct_result = llm.invoke(direct_prompt.messages)\n","print(\"Direct Result:\\n\", direct_result.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-4ZMJX3DI2q","executionInfo":{"status":"ok","timestamp":1751726783623,"user_tz":240,"elapsed":1281,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"8afdeb8b-599f-4670-ea79-6cda17dfa62a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Direct Result:\n"," Sarah buys 4 cupcakes at $3 each, totaling \\( 4 \\times 3 = 12 \\) dollars. She also buys 6 cookies at $2 each, totaling \\( 6 \\times 2 = 12 \\) dollars. The total cost of her purchase is \\( 12 + 12 = 24 \\) dollars. Paying with a $50 bill, she receives \\( 50 - 24 = 26 \\) dollars in change. Thus, Sarah will receive $26 in change.\n"]}]},{"cell_type":"markdown","source":["## 2C\n","I think the CoT result is more reliable. the result is \"Sarah will receive $26 in change.\""],"metadata":{"id":"4pTyFkdkDfVx"}},{"cell_type":"markdown","source":["Why is CoT usually more reliable?\n","1. Forces Step-by-Step Reasoning\n","CoT prompts explicitly tell the model to “think step by step.”\n","\n","This helps prevent the model from jumping straight to an answer (which is where it most often makes mistakes).\n","\n","2. Reduces \"Shortcut\" Errors\n","LLMs sometimes use pattern-matching shortcuts to guess an answer if asked directly.\n","\n","When you ask for steps, the model must “show its work,” making it less likely to overlook details.\n","\n","3. Reveals Model's Reasoning\n","You can see how the model got its answer.\n","\n","If there’s a mistake, it’s easier to spot (and fix) because you see all the steps.\n","\n","4. Activates Training Patterns\n","During training, LLMs saw a lot of “let’s think step by step” and similar reasoning patterns.\n","\n","CoT prompts match those patterns, so the model “knows” to be more careful and explicit.\n","\n","5. Works Especially Well for Multi-Step Tasks\n","The more steps required, the more CoT helps.\n","\n","For math word problems, logical deduction, and coding, CoT prompting is much more robust.\n","\n"],"metadata":{"id":"nwc3PCq-EIlm"}},{"cell_type":"markdown","source":["## 3A"],"metadata":{"id":"a3EHwXwyELku"}},{"cell_type":"code","source":["prompt_text = (\n","    \"Let's solve the following logic problem step by step:\\n\\n\"\n","    \"In a race with 5 participants (Alice, Bob, Carol, David, and Eve), \"\n","    \"Alice finished before Bob but after Carol. David finished last, and \"\n","    \"Eve finished before Alice but after Bob. What is the final ranking?\"\n",")\n","\n","prompt = ChatPromptTemplate.from_messages([\n","    (\"system\", \"Solve logic puzzles step by step. Show your reasoning and then give the final ranking as a list from 1st to 5th.\"),\n","    (\"human\", prompt_text)\n","])\n","\n","temperatures = [0.3, 0.7, 1.0]\n","results = []\n","\n","for temp in temperatures:\n","    llm = ChatOpenAI(model=\"gpt-4o\", temperature=temp)\n","    prompt_instance = prompt.invoke({})\n","    response = llm.invoke(prompt_instance.messages)\n","    print(f\"\\nTemperature: {temp}\\n{response.content}\\n\")\n","    results.append((temp, response.content))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cD_IW0MqEjpw","executionInfo":{"status":"ok","timestamp":1751727131030,"user_tz":240,"elapsed":17688,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"110ee62f-3b0f-46af-a181-09635cd96140"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Temperature: 0.3\n","To solve the puzzle, let's analyze the given information step by step:\n","\n","1. **Alice finished before Bob but after Carol.**\n","   - This means Carol finished before Alice, and Alice finished before Bob.\n","   - So, the order among these three is: Carol > Alice > Bob.\n","\n","2. **David finished last.**\n","   - David is in the 5th position.\n","\n","3. **Eve finished before Alice but after Bob.**\n","   - This means Bob finished before Eve, and Eve finished before Alice.\n","   - So, the order among these three is: Bob > Eve > Alice.\n","\n","Now, let's combine all these pieces of information:\n","\n","- From the first point, we have: Carol > Alice > Bob.\n","- From the third point, we have: Bob > Eve > Alice.\n","\n","Combining these, we need to find a sequence that satisfies all conditions:\n","\n","- Since Carol is before Alice and Bob is before Eve, and Eve is before Alice, the only possible order is:\n","  - Carol > Bob > Eve > Alice > David.\n","\n","Thus, the final ranking is:\n","1. Carol\n","2. Bob\n","3. Eve\n","4. Alice\n","5. David\n","\n","\n","Temperature: 0.7\n","To solve this logic puzzle, we need to arrange the participants in the order they finished the race based on the given conditions:\n","\n","1. **Alice finished before Bob**: This means Alice's position is better than Bob's position.\n","2. **Alice finished after Carol**: This means Carol's position is better than Alice's position.\n","3. **David finished last**: This means David is in the 5th position.\n","4. **Eve finished before Alice but after Bob**: This means Bob's position is better than Eve's position, and Eve's position is better than Alice's position.\n","\n","Let's break this down:\n","\n","- Since David finished last, he is in 5th place.\n","- We know that Carol finished before Alice, so Carol must be in a better position than Alice.\n","- Alice finished before Bob, so Alice's position is better than Bob's position.\n","- Eve finished before Alice but after Bob. This means the order must be Bob, Eve, Alice in terms of their finishing positions.\n","\n","Let's put these facts together:\n","\n","- Carol, Alice, and Bob must be in positions 1, 2, and 3 respectively because David is last (5th), and Eve must be between Bob and Alice.\n","- Since Carol is before Alice and Alice is before Bob, the order for Carol, Alice, and Bob must be Carol, Alice, Bob.\n","- Eve is positioned between Bob and Alice, so the only possibility is Carol, Bob, Eve, Alice.\n","\n","Thus, the final ranking based on these conditions is:\n","1. Carol\n","2. Bob\n","3. Eve\n","4. Alice\n","5. David\n","\n","Final ranking from 1st to 5th: Carol, Bob, Eve, Alice, David.\n","\n","\n","Temperature: 1.0\n","Let's solve the puzzle step by step by analyzing the given conditions:\n","\n","1. **Alice finished before Bob but after Carol.**\n","   - This puts the order of Alice, Bob, and Carol as: Carol > Alice > Bob.\n","\n","2. **David finished last.**\n","   - David is in 5th place.\n","\n","3. **Eve finished before Alice but after Bob.**\n","   - This means Eve is before Alice but after Bob. Since we know Alice is already ranked, we can place Eve: Bob > Eve > Alice.\n","\n","Combining these conditions:\n","- From point 1, we have: Carol > Alice > Bob\n","- From point 3, including Eve: Bob > Eve > Alice\n","\n","We need to integrate all the information:\n","- Eve is between Bob and Alice, so the order becomes: Carol > Bob > Eve > Alice\n","- And David is always last, making the complete sequence Carol > Bob > Eve > Alice > David.\n","\n","Final ranking:\n","1st: Carol\n","2nd: Bob\n","3rd: Eve\n","4th: Alice\n","5th: David\n","\n"]}]},{"cell_type":"markdown","source":["## 3C\n","All three temperature settings led to the same final ranking.\n","\n","The model's step-by-step reasoning correctly integrated all the logical constraints, even as temperature increased.\n","\n","Self-consistency is high in this case: the answer did not change with temperature, showing robustness.\n","\n"],"metadata":{"id":"btUc3SuRFY_q"}},{"cell_type":"markdown","source":["## 4\n","\n","\n","## A"],"metadata":{"id":"EpLS_8HvFfDq"}},{"cell_type":"code","source":["explore_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\",\n","     \"You are a creative camping expert. For the given constraints, brainstorm several different strategies for selecting the 5 most important items. For each, briefly explain your reasoning and then list your 5 chosen items.\"),\n","    (\"human\",\n","     \"You need to pack for a 3-day camping trip. You have a backpack that can hold 5 items maximum. \"\n","     \"Available items: tent, sleeping bag, food (3 days worth), water bottles, flashlight, first aid kit, map, extra clothes, cooking equipment. \"\n","     \"List a few different possible packing strategies. For each, explain your reasoning and list your 5 chosen items.\")\n","])\n","\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n","prompt_instance = explore_prompt.invoke({})\n","response = llm.invoke(prompt_instance.messages)\n","print(response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZYWkd1ZHhBZ","executionInfo":{"status":"ok","timestamp":1751727880883,"user_tz":240,"elapsed":8113,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"fc61b4e1-1c79-4d99-d7ec-1641a2038886"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Certainly! Here are a few different strategies for selecting the 5 most essential items for a 3-day camping trip:\n","\n","### Strategy 1: Survival Essentials\n","**Reasoning:** Prioritize the most critical items for basic survival, focusing on shelter, warmth, hydration, and food.\n","\n","1. **Tent** - Provides essential shelter and protection from the elements.\n","2. **Sleeping Bag** - Ensures warmth and comfort during the night.\n","3. **Water Bottles** - Crucial for staying hydrated throughout the trip.\n","4. **Food (3 days worth)** - Necessary to maintain energy and health.\n","5. **First Aid Kit** - Essential for handling any minor injuries or health issues.\n","\n","### Strategy 2: Lightweight and Efficient\n","**Reasoning:** Select items that are lightweight and multifunctional, maximizing efficiency and mobility.\n","\n","1. **Tent** - Offers shelter while being relatively compact.\n","2. **Sleeping Bag** - Lightweight and compact for easy carrying.\n","3. **Water Bottles** - Stay hydrated without the bulk.\n","4. **Food (3 days worth)** - Choose lightweight, non-perishable options.\n","5. **Flashlight** - Multi-use tool for visibility and safety at night.\n","\n","### Strategy 3: Comfort and Convenience\n","**Reasoning:** Focus on items that provide a comfortable and convenient camping experience.\n","\n","1. **Tent** - Provides a personal space and protection from the weather.\n","2. **Sleeping Bag** - Adds warmth and comfort for restful sleep.\n","3. **Extra Clothes** - Ensures comfort and adaptability to weather changes.\n","4. **Food (3 days worth)** - Essential for comfort and energy.\n","5. **Cooking Equipment** - Enhances meal options and enjoyment.\n","\n","### Strategy 4: Emergency Preparedness\n","**Reasoning:** Prepare for emergencies by prioritizing safety and navigation tools, alongside basic needs.\n","\n","1. **Tent** - Shelter is crucial for safety against elements.\n","2. **Sleeping Bag** - Important for warmth and rest.\n","3. **Water Bottles** - Hydration is a priority.\n","4. **First Aid Kit** - Essential for treating injuries.\n","5. **Map** - Important for navigation and avoiding getting lost.\n","\n","### Strategy 5: Balanced Approach\n","**Reasoning:** Balance between survival, comfort, and safety, ensuring a well-rounded camping experience.\n","\n","1. **Tent** - Critical for shelter.\n","2. **Sleeping Bag** - Necessary for warmth.\n","3. **Water Bottles** - Essential for hydration.\n","4. **Food (3 days worth)** - Necessary for sustenance.\n","5. **Flashlight** - Important for safety and convenience.\n","\n","These strategies provide various approaches to choosing the most crucial items for a 3-day camping trip, each tailored to different priorities and goals.\n"]}]},{"cell_type":"markdown","source":["## B"],"metadata":{"id":"cVy9tACSHimu"}},{"cell_type":"code","source":["tot_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\",\n","     \"You are an expert in planning camping trips. Given the constraints, list several different possible strategies to choose the best items, each based on a different priority. For each, explain your reasoning, then list the 5 chosen items.\"),\n","    (\"human\",\n","     \"You need to pack for a 3-day camping trip. You have a backpack that can hold 5 items maximum. Available items: tent, sleeping bag, food (3 days worth), water bottles, flashlight, first aid kit, map, extra clothes, cooking equipment.\\n\\nGenerate three possible packing plans:\\n1. Safety-first\\n2. Comfort-first\\n3. Minimal-weight\\n\\nFor each plan, briefly explain your reasoning and list your 5 chosen items.\")\n","])\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n","\n","prompt_instance = tot_prompt.invoke({})\n","response = llm.invoke(prompt_instance.messages)\n","print(response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATM_xTwkGa2g","executionInfo":{"status":"ok","timestamp":1751727686802,"user_tz":240,"elapsed":6540,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"06e2d4b5-4f3e-4985-b2de-2b292635ad1c"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["### Safety-first Plan:\n","**Reasoning:** The priority here is to ensure that you are prepared for any emergencies or unexpected situations that might arise during the trip. Items are selected based on their ability to provide shelter, help navigate, and deal with potential injuries.\n","\n","1. **Tent** - Provides shelter against the elements, which is crucial for safety.\n","2. **First Aid Kit** - Essential for treating injuries and ensuring health during the trip.\n","3. **Map** - Critical for navigation and ensuring you don't get lost.\n","4. **Food (3 days worth)** - Ensures sustenance and energy for the duration of the trip.\n","5. **Water Bottles** - Hydration is vital for survival and maintaining health.\n","\n","### Comfort-first Plan:\n","**Reasoning:** This plan focuses on maximizing comfort and convenience during the camping trip. The choices prioritize a pleasurable and relaxing experience.\n","\n","1. **Tent** - Offers comfort and protection while sleeping outdoors.\n","2. **Sleeping Bag** - Ensures a warm and comfortable night's sleep.\n","3. **Food (3 days worth)** - Provides nourishment and enjoyment with meals.\n","4. **Cooking Equipment** - Enhances meal preparation, adding to the comfort of the trip.\n","5. **Extra Clothes** - Allows for changes in weather and ensures comfort in various conditions.\n","\n","### Minimal-weight Plan:\n","**Reasoning:** The focus is on reducing the overall weight of the backpack, making the trip easier on the back and allowing for more agile movement. Items are chosen for their essential functionality with the least amount of bulk.\n","\n","1. **Sleeping Bag** - Lightweight and provides necessary warmth and comfort.\n","2. **Food (3 days worth)** - Compact and essential for nourishment.\n","3. **Water Bottles** - Crucial for hydration, with the possibility of refilling.\n","4. **Flashlight** - Provides light with minimal weight, useful for nights.\n","5. **First Aid Kit** - A lightweight version for treating injuries, prioritizing safety with minimal bulk.\n","\n","Each plan is tailored to a different priority, ensuring that you can choose based on what aspect is most important for your camping trip.\n"]}]},{"cell_type":"markdown","source":["## C analysis:\n","Part 1: Exploring Multiple Approaches (Unlabeled Branches)\n","The model brainstormed several strategies:\n","\n","Survival Essentials: Focused on shelter, warmth, hydration, food, first aid.\n","\n","Lightweight and Efficient: Chose items that are light and multifunctional, favoring a flashlight over first aid.\n","\n","Comfort and Convenience: Prioritized extra clothes and cooking equipment.\n","\n","Emergency Preparedness: Emphasized navigation (map) and first aid.\n","\n","Balanced Approach: A compromise including flashlight for convenience.\n","\n","Observation:\n","Each approach emerged naturally from the LLM’s “thinking,” and represents different real-world priorities—even though priorities were not stated explicitly. There is overlap in core items (tent, sleeping bag, food, water), but the fifth item changes based on the approach.\n","\n","Part 2: Explicit Branching by Priority\n","Safety-first:\n","\n","Reasoning: Prioritized emergencies, navigation, and survival.\n","\n","Items: Tent, first aid kit, map, food, water bottles.\n","\n","Comfort-first:\n","\n","Reasoning: Maximized sleeping comfort, meal enjoyment, and clothing changes.\n","\n","Items: Tent, sleeping bag, food, cooking equipment, extra clothes.\n","\n","Minimal-weight:\n","\n","Reasoning: Chose lightest essentials, avoided heavy or redundant items.\n","\n","Items: Sleeping bag, food, water bottles, flashlight, first aid kit.\n","\n","Observation:\n","Explicitly telling the model to “branch” by priority made the differences sharper and reasoning more systematic. For example, map appears in safety-first but not in comfort/minimal, and cooking equipment is only in comfort-first."],"metadata":{"id":"XqIbLv5uIDrn"}},{"cell_type":"markdown","source":["Comparison & Evaluation\n","Unlabeled branches capture diverse real-world strategies, showing the model’s creativity in problem-solving.\n","\n","Priority-labeled branches are clearer for decision-making and easier to compare, since reasoning for each priority is explicitly justified.\n","\n","Both approaches consistently included essentials (shelter, food, water), but differed in their 4th or 5th choice depending on the “thought branch.”\n","\n","Which approach is best?\n","\n","For real-world planning, priority-labeled ToT makes it easier to choose a plan based on your specific needs (safety, comfort, weight).\n","\n","The open-ended (unlabeled) ToT is better for discovering unexpected or creative solutions.\n","\n"],"metadata":{"id":"D0PLppE8ILp-"}},{"cell_type":"markdown","source":["## 5A"],"metadata":{"id":"-kg18uJdIRVT"}},{"cell_type":"code","source":["retrieval_content = \"\"\"\n","Solar:\n","- Advantages: Abundant and inexhaustible resource, minimal operating costs after installation, relatively low maintenance, and can be installed in various locations.\n","- Disadvantages: Intermittent energy production (dependent on sunlight), high initial installation costs, can be aesthetically unappealing, and requires significant land area.\n","\n","Wind:\n","- Advantages: Clean and renewable source, low operating costs, can be scaled for different needs, and can be located offshore.\n","- Disadvantages: Intermittent energy generation (dependent on wind), potential visual and noise pollution, and can impact wildlife.\n","\n","Hydro:\n","- Advantages: Reliable and steady if freshwater is available, can provide energy storage (via pumped storage), long lifespan. Most commercially available renewable source, reliable and flexible, can be used to power rural areas, and can be relatively inexpensive in the long run.\n","- Disadvantages: High upfront construction costs (dams, etc.), potential environmental impacts on aquatic habitats and local communities, and limited locations due to geographical requirements (e.g., consistent water flow).\n","\"\"\""],"metadata":{"id":"Vbj0pIBUJeex","executionInfo":{"status":"ok","timestamp":1751728626992,"user_tz":240,"elapsed":8,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## 5B"],"metadata":{"id":"giX_wMLVKQdd"}},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_openai import ChatOpenAI\n","\n","rag_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\",\n","     \"You are an expert energy consultant. Use only the information in the retrieved document below to answer the user's question.\"),\n","    (\"human\",\n","     f\"Retrieved document:\\n{retrieval_content}\\n\\nQuestion: What are the advantages and disadvantages of different renewable energy sources, and which would be most suitable for a small island nation?\")\n","])\n","\n","llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n","prompt_instance = rag_prompt.invoke({})\n","response_rag = llm.invoke(prompt_instance.messages)\n","print(response_rag.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gN2Lp30VKSMp","executionInfo":{"status":"ok","timestamp":1751728637459,"user_tz":240,"elapsed":7785,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"980f446c-ec7e-4ee9-8abe-8616ae8589c0"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["When considering renewable energy sources for a small island nation, it's important to weigh the advantages and disadvantages of each option:\n","\n","**Solar Energy:**\n","- *Advantages:* Solar energy is abundant and inexhaustible, with minimal operating costs after installation. It requires relatively low maintenance and can be installed in various locations, making it versatile for different settings.\n","- *Disadvantages:* Solar energy production is intermittent, as it depends on sunlight, and the initial installation costs can be high. It may also require significant land area and can be aesthetically unappealing.\n","\n","**Wind Energy:**\n","- *Advantages:* Wind energy is a clean and renewable source with low operating costs. It can be scaled to meet different energy needs and can be located offshore, which might be beneficial for an island nation.\n","- *Disadvantages:* Like solar, wind energy generation is intermittent, depending on wind availability. It can also cause visual and noise pollution and may impact wildlife.\n","\n","**Hydro Energy:**\n","- *Advantages:* Hydro energy is reliable and steady if freshwater is available, and it can provide energy storage through pumped storage. It has a long lifespan and is the most commercially available renewable source. It is reliable, flexible, and can be relatively inexpensive in the long run.\n","- *Disadvantages:* Hydro energy requires high upfront construction costs, such as building dams, and can have potential environmental impacts on aquatic habitats and local communities. It is also limited by geographical requirements, such as the need for consistent water flow.\n","\n","**Suitability for a Small Island Nation:**\n","For a small island nation, the choice of renewable energy source would depend on the specific geographical and environmental conditions. If the island has ample sunlight and available land, solar energy could be a viable option. If there are consistent wind patterns, especially offshore, wind energy could be suitable. However, if the island has freshwater resources and suitable geography for hydroelectric installations, hydro energy could provide a reliable and steady energy supply. Each option should be evaluated based on the island's specific resources, environmental impact considerations, and economic factors.\n"]}]},{"cell_type":"markdown","source":["## 5C"],"metadata":{"id":"B719QlPaKjd3"}},{"cell_type":"markdown","source":[],"metadata":{"id":"3xspOkTvHoJm"}},{"cell_type":"code","source":["no_rag_prompt = ChatPromptTemplate.from_messages([\n","    (\"system\",\n","     \"You are an expert energy consultant. Answer the user's question using your own knowledge.\"),\n","    (\"human\",\n","     \"What are the advantages and disadvantages of different renewable energy sources (solar, wind, hydro), and which would be most suitable for a small island nation?\")\n","])\n","\n","prompt_instance = no_rag_prompt.invoke({})\n","response_norag = llm.invoke(prompt_instance.messages)\n","print(response_norag.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9cs76ZwKiEj","executionInfo":{"status":"ok","timestamp":1751728722460,"user_tz":240,"elapsed":17659,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"4118d93c-1b14-4a4d-b7ce-6d439e979e9b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["When considering renewable energy sources for a small island nation, it's important to evaluate the advantages and disadvantages of each option and consider the specific characteristics and needs of the island. Here's a breakdown of solar, wind, and hydro energy:\n","\n","### Solar Energy\n","\n","**Advantages:**\n","- **Abundant Resource:** Solar energy is plentiful in many regions, especially in areas with high sun exposure.\n","- **Scalability:** Solar panels can be installed on a small scale (e.g., rooftops) or large scale (e.g., solar farms).\n","- **Low Operating Costs:** Once installed, solar panels require minimal maintenance and have no fuel costs.\n","- **Modular Installation:** Solar systems can be expanded easily as demand grows.\n","\n","**Disadvantages:**\n","- **Intermittency:** Solar power is dependent on sunlight, which can be inconsistent due to weather and day-night cycles.\n","- **Initial Costs:** The upfront cost of solar panels and installation can be high, although prices have been decreasing.\n","- **Space Requirements:** Large solar farms require significant land area, which might be limited on small islands.\n","\n","### Wind Energy\n","\n","**Advantages:**\n","- **High Energy Yield:** Wind turbines can generate significant amounts of electricity, especially in windy areas.\n","- **Low Operating Costs:** Similar to solar, wind energy has low operational and maintenance costs after installation.\n","- **Land Use:** Wind farms can coexist with agricultural land, allowing dual land use.\n","\n","**Disadvantages:**\n","- **Intermittency:** Wind energy is variable and depends on wind availability, which can be unpredictable.\n","- **Visual and Noise Impact:** Wind turbines can be considered unsightly and noisy, which might be a concern for tourism-dependent islands.\n","- **Wildlife Impact:** Turbines can pose risks to birds and bats.\n","\n","### Hydro Energy\n","\n","**Advantages:**\n","- **Reliable and Consistent:** Hydropower provides a stable and continuous energy supply, especially if a reservoir is used.\n","- **High Efficiency:** Hydropower plants have high conversion efficiency rates compared to other renewables.\n","- **Storage Capability:** Reservoirs can store water for use during peak demand times.\n","\n","**Disadvantages:**\n","- **Environmental Impact:** Dams and reservoirs can disrupt local ecosystems and displace communities.\n","- **Geographic Limitations:** Suitable sites for hydropower are limited and may not be available on small islands.\n","- **High Initial Costs:** Building dams and infrastructure can be expensive and time-consuming.\n","\n","### Suitability for a Small Island Nation\n","\n","For a small island nation, the choice of renewable energy source depends on several factors, including geographic location, climate, available resources, and economic considerations:\n","\n","- **Solar Energy** is often a good fit for islands with high solar insolation and limited land space, as it can be installed on rooftops and other available surfaces.\n","- **Wind Energy** can be suitable if the island has consistent wind patterns and can accommodate the visual and noise impact of turbines.\n","- **Hydro Energy** might be less feasible unless the island has significant freshwater resources and suitable topography for small-scale hydro projects.\n","\n","Ultimately, a combination of these renewable sources, possibly supplemented by energy storage solutions like batteries, might provide the most reliable and sustainable energy system for a small island nation. This approach can help mitigate the intermittency issues associated with solar and wind energy and provide a more balanced and resilient energy supply.\n"]}]},{"cell_type":"markdown","source":["## 5C\n","The main difference is that, first, RAG simulations gives an answer like \"we can decide which one to choice according to the features of the island.\" However, the pretrain model gives answer like \" we can have a combination of all of them.\" which might not be the answer we really want. Secondly, the RAG simualtion gives answer in a short way. It uses the text we provided to answer. While the pretrained model gives a very long answer. It trys to gather the information first."],"metadata":{"id":"OerbX6WmLGNS"}},{"cell_type":"markdown","source":["## 5D\n","Benefits of Using External Knowledge (RAG) in Prompt Engineering\n","Accuracy & Trust: RAG allows you to control the information the model can access, ensuring the answer is grounded in facts you provide.\n","\n","Freshness: Retrieval enables up-to-date knowledge, even if the LLM is out-of-date.\n","\n","Domain Adaptation: Easily add custom, domain-specific facts that may not be present in pre-trained weights.\n","\n","Transparency: You can audit the “retrieved” content, making it easy to debug and trust the reasoning.\n","\n","Safer Outputs: Reduces risk of hallucination, as the model is limited to retrieved facts.\n","\n"],"metadata":{"id":"uVQ8GGIAMOdN"}},{"cell_type":"markdown","source":["## 6A"],"metadata":{"id":"W9pmhOKLYXQp"}},{"cell_type":"code","source":["!pip install langchain langchain-openai google-search-results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2n2AFUkiLF8r","executionInfo":{"status":"ok","timestamp":1751749192190,"user_tz":240,"elapsed":4636,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"46343752-ca8b-4f87-e012-d5ba12f96fd8"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.27)\n","Collecting google-search-results\n","  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.67)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.93.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.67.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n","Building wheels for collected packages: google-search-results\n","  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32010 sha256=d691bb1639d71e647164f7bf6c764b32557f84bc752c192618884751ac3ca387\n","  Stored in directory: /root/.cache/pip/wheels/6e/42/3e/aeb691b02cb7175ec70e2da04b5658d4739d2b41e5f73cd06f\n","Successfully built google-search-results\n","Installing collected packages: google-search-results\n","Successfully installed google-search-results-2.4.2\n"]}]},{"cell_type":"code","source":["!pip install langchain_community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c97Pk5hjZR3u","executionInfo":{"status":"ok","timestamp":1751749312382,"user_tz":240,"elapsed":6664,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"d95274a9-de55-4251-d291-c504e8c276e6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_community\n","  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.67)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n","  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.4)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n","  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n","Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n","Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"]}]},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","from langchain.agents import initialize_agent, Tool\n","from langchain.agents.agent_types import AgentType\n","\n","\n"],"metadata":{"id":"Nu7q605FY5ho","executionInfo":{"status":"ok","timestamp":1751749370558,"user_tz":240,"elapsed":10,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from langchain_community.utilities import SerpAPIWrapper\n"],"metadata":{"id":"Qtoovqv6ZjNE","executionInfo":{"status":"ok","timestamp":1751749520131,"user_tz":240,"elapsed":152,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import os\n","os.environ[\"SERPAPI_API_KEY\"] = \"252adbec8485f8f8f5c74c7b847518c4caefdcf8d3fe0ac5906f77a62fe4f985\"\n"],"metadata":{"id":"2GSNnhZRbX7U","executionInfo":{"status":"ok","timestamp":1751750264147,"user_tz":240,"elapsed":6,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["from langchain_community.agent_toolkits.load_tools import load_tools\n","tools = load_tools([\"serpapi\"])"],"metadata":{"id":"rdKfNbwsaLA-","executionInfo":{"status":"ok","timestamp":1751750268215,"user_tz":240,"elapsed":3,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# 3. Initialize tools\n","search = SerpAPIWrapper()   # Web search tool\n","tools = load_tools(['serpapi', 'llm-math'], llm=ChatOpenAI())  # Includes search and calculator"],"metadata":{"id":"Ur3OmwdGdobQ","executionInfo":{"status":"ok","timestamp":1751750447777,"user_tz":240,"elapsed":40,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# 4. Initialize LLM\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # or \"gpt-4o\" if you have access\n","\n","# 5. Initialize agent with tools\n","agent = initialize_agent(\n","    tools,\n","    llm,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=True\n",")\n","\n","# 6. Multi-step question for the agent\n","question = (\n","    \"What is the population of the largest city in Japan, \"\n","    \"and what would be 15% of that population?\"\n",")\n","\n","# 7. Run the agent and print result\n","response = agent.run(question)\n","print(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HxDmeJRdHCi","executionInfo":{"status":"ok","timestamp":1751750521304,"user_tz":240,"elapsed":7188,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"1943f0ea-1f47-4790-fbaf-a534675c0d4a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-23-640519425.py:5: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n","  agent = initialize_agent(\n","/tmp/ipython-input-23-640519425.py:19: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n","  response = agent.run(question)\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3mI can use a search engine to find the population of the largest city in Japan and then a calculator to calculate 15% of that population. \n","Action: Search\n","Action Input: Population of largest city in Japan\u001b[0m\n","Observation: \u001b[36;1m\u001b[1;3m['Tokyo, with almost nine million inhabitants, is by far the largest Japanese city. (The Greater Tokyo region is estimated to have a population of some 13 million) ...', 'The largest city in Japan is Tokyo, with a population of 8,336,599 people. Despite being just the 61st largest country in the world by area, Japan is ranked ...', '1, Tokyo · Tokyo, 9,272,740, 11 ...', \"With approximately 9.57 million inhabitants, Tokyo was Japan's most populous city as of 2023, followed by Yokohama, which, in the same year, ...\", 'Tokyo, officially the Tokyo Metropolis, is the capital and most populous city in Japan. With a population of over 14 million in the city proper in 2023, ...', 'Tokyo, Japan, is the largest city on Earth, with a population of 37.4 million people, which is over four times the population of New York City, ...', 'All Cities in Japan ; 1, Tokyo, 14,038,000 ; 2, Kagoshima, 1,563,000 ...', \"The Tokyo metropolis is 13,452km2 which is tiny when you consider there's 37.4 million people there. Melbourne is a sprawling mess (the inner ...\"]\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3mI found that Tokyo is the largest city in Japan with a population of 8,336,599. Now I can use a calculator to find 15% of that population.\n","Action: Calculator\n","Action Input: 15% of 8,336,599\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3mAnswer: 1250489.8499999999\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m15% of the population of Tokyo, the largest city in Japan, is approximately 1,250,489.\n","Final Answer: 1,250,489\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n","1,250,489\n"]}]},{"cell_type":"markdown","source":["## 6B"],"metadata":{"id":"bzIS9lbzemNq"}},{"cell_type":"code","source":["\n","# Initialize the LLM (standalone, no tools)\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # Or \"gpt-4o\" if you have access\n","\n","# Standalone prompt (no tools or retrieval)\n","question = (\n","    \"What is the population of the largest city in Japan, and what would be 15% of that population? \"\n","    \"Please show your reasoning.\"\n",")\n","\n","# Ask the LLM directly\n","response = llm.invoke(question)\n","print(response.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtBFzYx6enm9","executionInfo":{"status":"ok","timestamp":1751750707045,"user_tz":240,"elapsed":1131,"user":{"displayName":"David Zhu","userId":"08893754115242056786"}},"outputId":"8e8b20a1-1c85-4a0a-c7c1-4afc6725dc8e"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["The largest city in Japan is Tokyo, with a population of approximately 9.73 million as of 2021.\n","\n","To find 15% of the population of Tokyo, we can use the formula:\n","\n","15% of 9.73 million = 0.15 x 9,730,000 = 1,459,500\n","\n","Therefore, 15% of the population of Tokyo would be approximately 1,459,500 people.\n"]}]},{"cell_type":"markdown","source":["Tool-Augmented LLMs (e.g., ReAct agent with SerpAPI + Calculator)\n","Advantages:\n","\n","Access to Real-Time and External Data:\n","The LLM can fetch up-to-date information from the web, databases, or perform live calculations.\n","Example: Your agent fetched the actual current population of Tokyo using a search engine, then calculated 15% programmatically.\n","\n","Increased Accuracy:\n","Answers are grounded in real data, reducing errors due to outdated or memorized knowledge.\n","\n","Enhanced Capabilities:\n","Can solve multi-step or tool-based tasks that go beyond the LLM’s own “memory” (e.g., currency conversions, mathematical calculations, code execution).\n","\n","Transparency in Reasoning:\n","The reasoning chain and tool usage steps are visible, allowing for easier debugging and validation.\n","\n","Limitations:\n","\n","Dependency on Tool Availability:\n","Requires integration and access to APIs or external tools, which might need keys, credits, or may go offline.\n","\n","Latency:\n","Querying external tools can slow down response time compared to instant LLM answers.\n","\n","Complexity:\n","Setting up agents, tools, and API credentials adds engineering overhead.\n","\n","Standalone Prompting (LLM Only, No Tools)\n","Advantages:\n","\n","Simplicity:\n","No extra setup or integration—just a single prompt to the model.\n","\n","Speed:\n","The model generates a response instantly, without waiting for external API calls.\n","\n","Low Overhead:\n","No need to manage API keys, credits, or tool-specific code.\n","\n","Limitations:\n","\n","Stale or Inaccurate Knowledge:\n","The LLM relies on its training data, which may be outdated (e.g., population numbers from 2021 or earlier).\n","\n","Estimates, Not Facts:\n","Answers may be “best guesses” based on its training, not live data. This can lead to errors, especially in fast-changing domains (e.g., news, prices, population).\n","\n","No Programmatic Calculation:\n","For arithmetic or logic, the model simulates reasoning but can make mistakes or hallucinate.\n","\n"],"metadata":{"id":"yLkyxnfpe_uv"}},{"cell_type":"markdown","source":["Tool-Augmented:\n","Gave the latest population (8,336,599 for Tokyo), used a calculator tool, and output: 1,250,489 (15% of that).\n","\n","Standalone:\n","Used its memorized population (9.73 million, as of 2021), calculated 15% as 1,459,500."],"metadata":{"id":"UmpcAjsVfC6I"}},{"cell_type":"code","source":[],"metadata":{"id":"SzSCd9ZTfCXg"},"execution_count":null,"outputs":[]}]}